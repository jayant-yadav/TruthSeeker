{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install --no-cache-dir torch==2.2.2 torchaudio==2.2.2 \n",
    "# %pip install whisperx --no-cache-dir\n",
    "# %pip install sounddevice --no-cache-dir #scipy wave #numpy==2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniforge/base/envs/impacthack_whisper_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../usr/local/Caskroom/miniforge/base/envs/impacthack_whisper_env/lib/python3.10/site-packages/whisperx/assets/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.2. Bad things might happen unless you revert torch to 1.x.\n",
      "Recording 10 seconds of audio...\n",
      "Recording saved.\n",
      "Transcribing audio...\n",
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: en (0.98) in first 30s of audio...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import whisperx\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wave\n",
    "import tempfile\n",
    "\n",
    "# Configuration\n",
    "device = \"cpu\"  # macOS does not support CUDA\n",
    "compute_type = \"int8\"#\"float32\"  # Use float32 instead of float16\n",
    "model_size = \"base\" #large-v2\"\n",
    "sample_rate = 16000  # Whisper expects 16kHz audio\n",
    "duration = 10  # Recording duration in seconds\n",
    "\n",
    "# Load WhisperX model (force float32 on macOS)\n",
    "model = whisperx.load_model(model_size, device, compute_type=compute_type)\n",
    "\n",
    "# Temporary file for recording\n",
    "temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "temp_wav_path = temp_wav.name\n",
    "\n",
    "def record_audio(filename, duration, sample_rate):\n",
    "    \"\"\"Records audio from the microphone and saves it as a WAV file.\"\"\"\n",
    "    print(f\"Recording {duration} seconds of audio...\")\n",
    "    audio_data = sd.rec(int(sample_rate * duration), samplerate=sample_rate, channels=1, dtype=np.int16)\n",
    "    sd.wait()  # Wait for recording to finish\n",
    "    wavefile = wave.open(filename, \"wb\")\n",
    "    wavefile.setnchannels(1)\n",
    "    wavefile.setsampwidth(2)\n",
    "    wavefile.setframerate(sample_rate)\n",
    "    wavefile.writeframes(audio_data.tobytes())\n",
    "    wavefile.close()\n",
    "    print(\"Recording saved.\")\n",
    "\n",
    "\n",
    "# Record audio\n",
    "record_audio(temp_wav_path, duration, sample_rate)\n",
    "\n",
    "\n",
    "# Transcribe using WhisperX\n",
    "print(\"Transcribing audio...\")\n",
    "audio = whisperx.load_audio(temp_wav_path)\n",
    "transcription = model.transcribe(audio)\n",
    "\n",
    "# Load diarization pipeline\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=\"HF_API_KEY\", device=device)\n",
    "diarized_segments = diarize_model(temp_wav_path)\n",
    "\n",
    "# Align transcription with diarization\n",
    "aligned_transcription = whisperx.align(transcription[\"segments\"], diarized_segments, model.lang, model_size, device)\n",
    "\n",
    "# Print diarized transcription\n",
    "for segment in aligned_transcription[\"segments\"]:\n",
    "    speaker = segment.get(\"speaker\", \"Unknown\")\n",
    "    print(f\"[{speaker}] {segment['text']}\")\n",
    "\n",
    "print(\"Transcription with diarization complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "impacthack_whisper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
